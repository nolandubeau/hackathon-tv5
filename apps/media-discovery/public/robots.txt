# robots.txt
# Generated by ARW CLI
# https://github.com/agent-ready-web/agent-ready-web

# Standard Web Crawlers
User-agent: *
Allow: /

# AI Training Agents - Training Not Allowed
# These agents are blocked from crawling for model training
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: Google-Extended
User-agent: CCBot
User-agent: anthropic-ai
User-agent: Claude-Web
User-agent: Omgilibot
User-agent: FacebookBot
Disallow: /

# AI Inference Agents - Real-time Query Answering
User-agent: ChatGPT-User
User-agent: PerplexityBot
User-agent: ClaudeBot
User-agent: Applebot-Extended
User-agent: Bytespider
Allow: /

# Agent-Ready Web Discovery
# For ARW-compliant agents, see /llms.txt for structured discovery
# Specification: https://github.com/agent-ready-web/agent-ready-web
#
# ARW provides:
#  - Structured content discovery via /llms.txt
#  - Machine-readable content views (.llm.md files)
#  - OAuth-protected actions for transactions
#  - Machine-readable policies and rate limits
#
# Sitemap: /sitemap.xml
